{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Sentiment Model using ESPPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](http://sww.sas.com/saspediawiki/images/4/41/Esppy_pipeline_amazon_review.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esp.delete_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esp.get_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Connect to ESP server and create project locally\n",
    "# import sys\n",
    "# sys.path.insert(0, './python-esp')\n",
    "import esppy\n",
    "esp = esppy.ESP('espserver')\n",
    "proj = esp.create_project('amazon_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('reviews_train_5000.csv', header=None, \n",
    "                             names=[\"id\", \"title\", \"content\", \"rank\", \"sentiment\"])\n",
    " \n",
    "score_data = pd.read_csv('reviews_test_1000.csv', header=None, \n",
    "                             names=[\"id\", \"title\", \"content\", \"rank\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentiment'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is an imbalance in the training set. In order to build a generalizable model, sample the data for each stratum and build a new table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "n_samples = 5000\n",
    "train_ratio = 0.5\n",
    "\n",
    "# get all the rows in train where sentiment is 1 \n",
    "# return a random sample of 2500 rows from this subset \n",
    "train_data_sample_pos = train_data.loc[train_data['sentiment'] == 1.0].sample(int(n_samples * train_ratio),\n",
    "                                                                              random_state=seed)\n",
    "# get all the rows in train where sentiment is 0\n",
    "# return a random sample of 2500 rows from this subset \n",
    "# sample with replacement gives duplicate rows but artifically increases rare cases\n",
    "train_data_sample_neg = train_data.loc[train_data['sentiment'] == 0.0].sample(int(n_samples * (1 - train_ratio)), \n",
    "                                                                              replace=True, random_state=seed)\n",
    "\n",
    "train_data_sample = pd.concat([train_data_sample_pos, train_data_sample_neg])\n",
    " \n",
    "train_data_sample_shuffled = shuffle(train_data_sample, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample_shuffled['sentiment'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_sample_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create source and calculate windows for data streams\n",
    "All event streams must enter continuous queries by being published or injected into a source window. Event streams cannot be published or injected into any other window type. Here the goal is to add a Source window to our project that will allow access to streaming data, preprocess the data, and result in a streaming dataset we can use to extract sentiment from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What data do we need to determine sentiment\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp.calculate.TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data source window\n",
    "src = esp.SourceWindow(schema=('id*:int64', 'content:string', 'sentiment:string'),\n",
    "                      index_type='empty', insert_only=True, autogen_key=True)\n",
    "\n",
    "# Create a calculation window to tokenize text input\n",
    "# Each token represents an event, output would be |id|tid|word\n",
    "tok = esp.calculate.Tokenization(schema=('id*:int64', 'tid*:int64', 'word:string'),\n",
    "                                 input_map=dict(docId='id', doc='content'),\n",
    "                                 output_map=dict(docIdOut='id*', tokenIdOut='tid*', wordOut='word'))\n",
    "\n",
    "# Create a calculation window to vectorize input tokens\n",
    "# TextVectorization will output a vector representation of the tokens\n",
    "# Optionally using a start and stop list to filter tokens\n",
    "vec = esp.calculate.TextVectorization(schema=('id*:int64', 'vector:array(dbl)'),\n",
    "                                     wordVec='/root/glove.6B.200d.txt',\n",
    "                                     startList='/root/pos_neg_words.txt',\n",
    "                                     stopList='/root/stop-words.txt',\n",
    "                                     wordVecDelimiter='SPACE',\n",
    "                                     outputDocVec=1,\n",
    "                                     input_map=dict(docId='id', token='word'),\n",
    "                                     output_map=dict(docIdOut='id*', vectorOut='vector[1-200]'))\n",
    "\n",
    "# Create a join window to join the source stream and \n",
    "# document vector for each row.\n",
    "# Want to include only sentiment, content, and word vector\n",
    "jn = esp.JoinWindow(type='inner')\n",
    "jn.add_condition(left='id', right='id')\n",
    "jn.add_field_selection(name='sentiment', source='l_sentiment')\n",
    "jn.add_field_selection(name='content', source='l_content')\n",
    "jn.add_field_selection(name='vector', source='r_vector')\n",
    "\n",
    "# Add the created windows to the project\n",
    "proj.windows['w_data'] = src\n",
    "proj.windows['w_tok'] = tok\n",
    "proj.windows['w_vec'] = vec\n",
    "proj.windows['w_join'] = jn\n",
    "\n",
    "# Connect the windows to create a pipeline\n",
    "src.add_target(tok, role='data')\n",
    "tok.add_target(vec, role='data')\n",
    "src.add_target(jn, role='data')\n",
    "vec.add_target(jn, role='data')\n",
    "\n",
    "# Plot the graph of project so far\n",
    "proj.to_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set we need to preprocess it the same as above, so we can create a second src window and 2nd calculation windows to create a similar pipeline that will feed in to the Project later in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data source window\n",
    "src2 = esp.SourceWindow(schema=('id*:int64', 'content:string', 'sentiment:string'),\n",
    "                      index_type='empty', insert_only=True, autogen_key=True)\n",
    "\n",
    "# Create a calculation window to tokenize text input\n",
    "# Each token represents an event, output would be |id|tid|word\n",
    "tok2 = esp.calculate.Tokenization(schema=('id*:int64', 'tid*:int64', 'word:string'),\n",
    "                                 input_map=dict(docId='id', doc='content'),\n",
    "                                 output_map=dict(docIdOut='id*', tokenIdOut='tid*', wordOut='word'))\n",
    "\n",
    "# Create a calculation window to vectorize input tokens\n",
    "# TextVectorization will output a vector representation of the tokens\n",
    "# Optionally using a start and stop list to filter tokens\n",
    "vec2 = esp.calculate.TextVectorization(schema=('id*:int64', 'vector:array(dbl)'),\n",
    "                                     wordVec='/root/glove.6B.200d.txt',\n",
    "                                     startList='/root/pos_neg_words.txt',\n",
    "                                     stopList='/root/stop-words.txt',\n",
    "                                     wordVecDelimiter='SPACE',\n",
    "                                     outputDocVec=1,\n",
    "                                     input_map=dict(docId='id', token='word'),\n",
    "                                     output_map=dict(docIdOut='id*', vectorOut='vector[1-200]'))\n",
    "\n",
    "\n",
    "jn2 = esp.JoinWindow(type = 'inner')\n",
    "jn2.add_condition(left='id', right='id')\n",
    "jn2.add_field_selection(name='sentiment', source='l_sentiment')\n",
    "jn2.add_field_selection(name='content', source='l_content')\n",
    "jn2.add_field_selection(name='vector', source='r_vector')\n",
    " \n",
    "proj.windows['w_data2'] = src2\n",
    "proj.windows['w_tok2']  = tok2\n",
    "proj.windows['w_vec2']  = vec2\n",
    "proj.windows['w_join2'] = jn2\n",
    " \n",
    "src2.add_target(tok2, role='data')\n",
    "tok2.add_target(vec2, role='data')\n",
    "src2.add_target(jn2, role='data')\n",
    "vec2.add_target(jn2, role='data')\n",
    " \n",
    "proj.to_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result to the project is two seperate streams that perform essentially the same task, albeit on different source data. The data is now in the correct format to train a model. Two models will be used - a logistic regression and a support vector machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create training and scoring windows for logistic regression and SVM\n",
    "\n",
    "Training windows provide the ability to train machine learning algorithms on streams of data. Models used in this fashion would be considered online models. Both training from scratch as well as sequentially updating the model on streams of data is possible. In this case the models will be trained as data is fed into the system. \n",
    "\n",
    "Scoring windows take the learned model and input data to generate predictions. This will add a column of predicted values to the output schema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training window\n",
    "# Specify indepedendent and dependent vars\n",
    "# nInit - Number of events to initilize \n",
    "# commitInterval - Number of events to process before commiting a model to downstream scoring \n",
    "# numC - Specifies the number of regularization parameters to try\n",
    "# ratioC - Specifies the ratio in setting the set of regularization parameters.\n",
    "train_lr = esp.train.LogisticRegression(input_map=dict(inputs=['vector[1-200]', 'sentiment'], target='sentiment'), \n",
    "                                        nInit=1500, commitInterval=500, numC=20, ratioC=2.5)\n",
    "\n",
    "# Add window to project\n",
    "proj.windows['w_train_lr'] = train_lr\n",
    "\n",
    "# Connect Join to LR\n",
    "jn.add_target(train_lr, role='data')\n",
    "\n",
    "# Create scoring window\n",
    "# Output predicted_y is P(sentiment=1)\n",
    "score_lr = esp.score.LogisticRegression(input_map=dict(inputs=['vector[1-200]']), \n",
    "                                        output_map=dict(yPredictOut='predicted_y', modelIdOut='model_id'),\n",
    "                                        schema=['id*:int64', 'content:string', 'sentiment:string', \n",
    "                                                'vector:array(dbl)', 'predicted_y:double', 'model_id:int64'])\n",
    "\n",
    "# Add score window to project\n",
    "proj.windows['w_score_lr'] = score_lr\n",
    "\n",
    "# Connect the test set to the score window\n",
    "jn2.add_target(score_lr, role='data')\n",
    "\n",
    "# Connect train window to score (score needs to use learned model built in train)\n",
    "train_lr.add_target(score_lr, role='model')\n",
    "\n",
    "# Display project\n",
    "esppy.options.display.image_scale = 0.65\n",
    "proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**\n",
    "\n",
    "Using same esp model parameters and identical graph as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm = esp.train.SVM(input_map=dict(inputs=['vector[1-200]', 'sentiment'], \n",
    "                                         target='sentiment'),\n",
    "                          nInit=1500, commitInterval=500,\n",
    "                          numC=20, ratioC=2.5)\n",
    "proj.windows['w_train_svm'] = train_svm\n",
    "\n",
    "jn.add_target(train_svm, role='data')\n",
    "\n",
    "score_svm = esp.score.SVM(input_map=dict(inputs='vector[1-200]'),\n",
    "                          output_map=dict(yPredictOut='predicted_y', modelIdOut='model_id'),\n",
    "                          schema=['id*:int64', 'content:string', 'sentiment:string',\n",
    "                                  'vector:array(dbl)', 'predicted_y:double', 'model_id:int64'])\n",
    " \n",
    "proj.windows['w_score_svm'] = score_svm\n",
    "\n",
    "jn2.add_target(score_svm, role='data')\n",
    "\n",
    "train_svm.add_target(score_svm, role='model')\n",
    "\n",
    "proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Online model fit statistics\n",
    "Treating this as a binary classification problem where sentiment is either 1 (for happy) or 0 (for sad). The output prediction is the probability that an event is 1. So anything above 0.5 would be predicted to be happy, otherwise it is sad. The performance of both models are based on MCE. \n",
    "\n",
    "To output fit statistics you can use a compute window and a fitstat window. The compute window is used to one-hot encode the target variable and generate, on a per event basis, the probability for both class predictions P(1) and P(0). The fitstat window is used to calculate the MCE from these probabilities and the ground truth label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute window to calculate output probabilities \n",
    "comp_lr = esp.ComputeWindow(schema=['id*:int64', 'sentiment:string',\n",
    "                                       'predicted_y:double', 'p_1:double', 'p_0:double'])\n",
    "\n",
    "# One-hot encode the true label \n",
    "comp_lr.add_field_expression(\"tostring(tointeger(sentiment))\")\n",
    "# Calulate class prob. predictions\n",
    "comp_lr.add_field_expression(\"predicted_y\")\n",
    "comp_lr.add_field_expression(\"predicted_y\")\n",
    "comp_lr.add_field_expression(\"1-predicted_y\")\n",
    "\n",
    "# Create fitstat window to calculate MCE\n",
    "fitstat_lr = esp.calculate.FitStat(schema=('id*:int64','mceOut:double'),\n",
    "                                      classLabels='0,1', windowLength=200)\n",
    "fitstat_lr.set_inputs(inputs=('p_0:double', 'p_1:double'), \n",
    "                         response=('sentiment:string'))\n",
    "fitstat_lr.set_outputs(mceOut='mceOut:double')\n",
    "\n",
    "# Add windows to project\n",
    "proj.windows['w_comp_lr'] = comp_lr\n",
    "proj.windows['w_fitstat_lr'] = fitstat_lr\n",
    "score_lr.add_target(comp_lr, role='data')\n",
    "comp_lr.add_target(fitstat_lr, role='data')\n",
    " \n",
    "esppy.options.display.image_scale = 0.45\n",
    "proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_svm = esp.ComputeWindow(\"w_comp_svm\", \n",
    "                               schema=['id*:int64', 'sentiment:string',\n",
    "                                       'predicted_y:double', 'p_1:double', 'p_0:double'])\n",
    "comp_svm.add_field_expression(\"tostring(tointeger(sentiment))\")\n",
    "comp_svm.add_field_expression(\"predicted_y\")\n",
    "comp_svm.add_field_expression(\"\"\"\n",
    "1/(1+pow(2.718281828, -predicted_y))\n",
    "\"\"\")\n",
    "comp_svm.add_field_expression(\"\"\"\n",
    "1-1/(1+pow(2.718281828, -predicted_y))\n",
    "\"\"\")\n",
    " \n",
    "fitstat_svm = esp.calculate.FitStat(schema=('id*:int64','mceOut:double'),\n",
    "                                    classLabels='0,1',\n",
    "                                    windowLength=200)\n",
    "fitstat_svm.set_inputs(inputs=('p_0:double', 'p_1:double'), \n",
    "                       response=('sentiment:string'))\n",
    "fitstat_svm.set_outputs(mceOut='mceOut:double')\n",
    " \n",
    "proj.windows['w_comp_svm'] = comp_svm\n",
    "proj.windows['w_fitstat_svm'] = fitstat_svm\n",
    "score_svm.add_target(comp_svm, role='data')\n",
    "comp_svm.add_target(fitstat_svm, role='data')\n",
    "esppy.options.display.image_scale = 0.45\n",
    "proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two fitstat windows for plotting\n",
    "jn3 = esp.JoinWindow(type = 'inner')\n",
    "jn3.add_condition(left='id', right='id')\n",
    "jn3.add_field_selection(name='mce_lr', source='l_mceOut')\n",
    "jn3.add_field_selection(name='mce_svm',   source='r_mceOut')\n",
    " \n",
    "proj.windows['w_join3'] = jn3\n",
    "fitstat_lr.add_target(jn3, role='data')\n",
    "fitstat_svm.add_target(jn3, role='data')\n",
    " \n",
    "#print(proj.to_xml(pretty=True))\n",
    "esppy.options.display.image_scale = 0.4\n",
    "proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esp.load_project(proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Publish and Subscribe to windows\n",
    "ESP windows works as a pub/sub model, meaning that some windows (like source) will be published too while other windows are subscribed too in order to see the output. In this project the data is streamed into the source window (w_data) through publishing and the remaining windows can be subscribed too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# esp.load_project(proj)\n",
    "\n",
    "# # stream the training data into the engine\n",
    "src.publish_events(train_data_sample_shuffled, pause=15)\n",
    " \n",
    "# subscribe necessary windows\n",
    "src.subscribe()\n",
    "tok.subscribe()\n",
    "vec.subscribe()\n",
    "jn.subscribe()\n",
    "train_lr.subscribe()\n",
    "score_lr.subscribe()\n",
    "train_svm.subscribe()\n",
    "score_svm.subscribe()\n",
    "comp_lr.subscribe()\n",
    "comp_svm.subscribe()\n",
    "fitstat_lr.subscribe()\n",
    "fitstat_svm.subscribe()\n",
    "jn3.subscribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following code display the outputs of the windows\n",
    " \n",
    "# src.tail(10)\n",
    "# tok.tail(10)\n",
    "vec.head()\n",
    "# jn.tail(10)\n",
    " \n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# train_lr.tail(10) # a list published logistic regression models\n",
    " \n",
    "# pandas.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# train_svm.tail(10) # a list published svm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
